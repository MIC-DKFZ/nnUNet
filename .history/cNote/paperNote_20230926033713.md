

# nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation
# nnU-Net: 自适应U-Net医学图像分割框架

> Fabian Isensee, Jens Petersen, Andre Klein, David Zimmerer, Paul F. Jaeger,
> Simon Kohl, Jakob Wasserthal, Gregor K¨ohler, Tobias Norajitra, Sebastian
> Wirkert, and Klaus H. Maier-Hein
> Division of Medical Image Computing, German Cancer Research Center (DKFZ),
> Heidelberg, Germany

## 摘要

U-Net是2015年提出的。由于其直接的成功架构，它很快就成为医学图像分割中常用的基准。然而，U-Net对新问题的适应包含了关于精确架构、预处理、训练和推理的几个自由度。这些选择彼此不独立，并且对整体性能和泛化能力有着重大影响。本文介绍了nnU-Net（“no-new-Net”），它是基于2D和3D香草U-Net的稳健和自适应框架。我们认为，许多提出的网络设计的超级钟和哨声是多余的，而是专注于剩余的方面，这些方面构成了方法的性能和泛化能力。我们在医学分割十项全能挑战赛的背景下评估了nnU-Net，该挑战赛衡量了十个学科中的分割性能，包括不同的实体、图像模态、图像几何和数据集大小，不允许数据集之间的手动调整。在提交稿时，nnU-Net在挑战的在线排行榜中实现了所有类别和七个阶段1任务（除了BrainTumour中的类别1）的最高平均骰子分数。

## 关键词: 语义分割、医学成像、U-Net

## 1. 引言

医学图像分割目前主要由深度卷积神经网络（CNNs）进行。然而，每个细分基准似乎都需要专门的架构和训练方案修改，以实现有竞争力的性能[1，2，3，4，5]。这导致了该领域大量的出版物，加上通常只在少数甚至只有一个数据集上进行有限的验证，使得研究人员越来越难以确定在有限的场景之外达到其承诺优势的方法。医学分割十项全能旨在专门解决这一问题：该挑战的参与者被要求创建一种分割算法，该算法可在与人体不同实体相对应的10个数据集中进行推广。这些算法可以动态地适应特定数据集的具体情况，但仅允许以完全自动的方式这样做。挑战分为两个连续的阶段：1）开发阶段，参与者可以访问7个数据集，以优化他们的方法，并且必须使用他们最终的、因此被冻结的方法，提交相应的7个测试集的分段。2） 第二阶段在3个先前未公开的数据集上评估相同的精确方法。

我们假设，最近提出的一些架构修改在一定程度上过度适应了特定的问题，或者可能由于对现有技术的次优重新实现而导致验证不完善。例如，使用U-Net作为内部数据集的基准，需要对该方法进行调整以适应新问题。这跨越了几个自由度。尽管体系结构本身是非常直接的，尽管该方法通常被用作基准，但我们认为，关于确切的体系结构、预处理、训练、推理和后处理的剩余相互依赖的选择往往会导致U-Net在用作基准时表现不佳。此外，如果网络还没有针对手头的任务进行完全优化，那么旨在提高网络性能的架构调整可以很容易地证明是有效的，从而为调整提供足够的空间来改善结果。然而，在我们自己的初步实验中，这些调整无法在完全优化的网络中改善分割结果，因此很可能无法提升现有技术。这让我们相信，非架构方面在分割方法中的影响要大得多，但同时也被严重低估了。
在本文中，我们提出了nnU-Net（“无新网”）框架。它驻留在一组三个相对简单的U-Net模型上，这些模型只包含对原始U-Net的微小修改[6]。我们省略了最近提出的扩展，例如使用残差连接[7，8]、密集连接[5]或注意力机制[4]。nnU-Net会根据给定的图像几何结构自动调整其架构。更重要的是，nnU-Net框架彻底定义了围绕它们的所有其他步骤。这些步骤可以获得或分别损失网络的大部分性能：预处理（例如: 重新采样和规范化）、训练（例如丢失、优化器设置和数据扩充）、推理（例如基于补丁的策略和跨测试时间扩充和模型的集成）以及潜在的后处理（例如，如果适用，强制执行单个连接组件）。

## 2. 方法

### 2.1 网络架构

医学图像通常包含第三维度，这就是为什么我们考虑由2D U-Net、3D U-Net和U-Net级联组成的基本U-Net架构池。当2D和3D U-Nets以全分辨率生成分割时，级联首先生成低分辨率分割，然后对其进行细化。与U-Net的原始公式相比，我们的架构修改几乎可以忽略不计，相反，我们专注于为这些模型设计自动训练管道。

  U-Net[6]是一个成功的编码器-解码器网络，近年来受到了广泛关注。它的编码器部分的工作原理类似于传统的分类CNN，因为它以减少的空间信息为代价，连续地聚合语义信息。由于在分割中，语义和空间信息对网络的成功至关重要，因此必须以某种方式恢复丢失的空间信息。U-Net通过解码器来实现这一点，解码器从“U”的底部接收语义信息，并将其与通过跳过连接直接从编码器获得的更高分辨率特征图重新组合。与其他分割网络不同，如FCN[9]和DeepLab[10]的先前迭代，这允许U-Net特别好地分割精细结构。
  就像最初的U-Net一样，我们在编码器中的池和解码器中的转置卷积操作之间使用两个普通卷积层。
我们偏离了原始架构，因为我们用泄漏的ReLU（负斜率1e−2）代替了ReLU激活函数，并使用实例规范化[11]而不是更流行的批处理规范化[12]。
2D U-Net直观地说，在3D医学图像分割的背景下使用2D U-Net似乎是次优的，因为沿着z轴的有价值的信息无法聚合和考虑。然而，有证据[13]表明，如果数据集是各向异性的，则传统的3D分割方法的性能会恶化（参见迪卡侬挑战赛的前列腺数据集）。
3D U-Net 3D U-Net似乎是3D图像数据的合适选择方法。在理想的世界里，我们会在整个患者的图像上训练这样的架构。然而，在现实中，我们受到可用GPU内存数量的限制，这使我们只能在图像补丁上训练这种架构。
虽然这对于由较小图像组成的数据集（就每位患者的体素数量而言）来说不是问题，例如本次挑战的脑肿瘤、海马和前列腺数据集，但由具有较大图像的数据集如肝脏所指示的基于补丁的训练可能会阻碍训练。这是由于架构的视野有限，因此无法收集足够的上下文信息来例如正确区分肝脏的部分和其他器官的部分。
U-Net级联为了解决3D U-Net在具有大图像大小的数据集上的这一实际缺点，我们还提出了一个级联模型。因此，首先在下采样图像上训练3D U-Net（阶段1）。然后，该U-Net的分割结果被上采样到原始体素间距，并作为额外的（一个热编码的）输入通道传递到第二个3D U-Net，该第二个三维U-Net在全分辨率的补丁上进行训练（阶段2）。见图1。


2 Methods
2.1 Network architectures
Medical images commonly encompass a third dimension, which is why we consider a pool of basic U-Net architectures consisting of a 2D U-Net, a 3D U-Net
and a U-Net Cascade. While the 2D and 3D U-Nets generate segmentations
at full resolution, the cascade first generates low resolution segmentations and
subsequently refines them. Our architectural modifications as compared to the
U-Net’s original formulation are close to negligible and instead we focus our
efforts on designing an automatic training pipeline for these models.
The U-Net [6] is a successful encoder-decoder network that has received a lot
of attention in the recent years. Its encoder part works similarly to a traditional
classification CNN in that it successively aggregates semantic information at the
expense of reduced spatial information. Since in segmentation, both semantic as
well as spatial information are crucial for the success of a network, the missing
spatial information must somehow be recovered. The U-Net does this through
the decoder, which receives semantic information from the bottom of the ’U’
and recombines it with higher resolution feature maps obtained directly from
the encoder through skip connections. Unlike other segmentation networks, such
as FCN [9] and previous iterations of DeepLab [10] this allows the U-Net to
segment fine structures particularly well.
Just like the original U-Net, we use two plain convolutional layers between
poolings in the encoder and transposed convolution operations in the decoder.
We deviate from the original architecture in that we replace ReLU activation
functions with leaky ReLUs (neg. slope 1e
−2
) and use instance normalization [11]
instead of the more popular batch normalization [12].
2D U-Net Intuitively, using a 2D U-Net in the context of 3D medical image segmentation appears to be suboptimal because valuable information along
the z-axis cannot be aggregated and taken into consideration. However, there
is evidence [13] that conventional 3D segmentation methods deteriorate in performance if the dataset is anisotropic (cf. Prostate dataset of the Decathlon
challenge).
3D U-Net A 3D U-Net seems like the appropriate method of choice for 3D
image data. In an ideal world, we would train such an architecture on the entire
patient’s image. In reality however, we are limited by the amount of available
GPU memory which allows us to train this architecture only on image patches.
While this is not a problem for datasets comprised of smaller images (in terms
of number of voxels per patient) such as the Brain Tumour, Hippocampus and
Prostate datasets of this challenge, patch-based training, as dictated by datasets
with large images such as Liver, may impede training. This is due to the limited
field of view of the architecture which thus cannot collect sufficient contextual
information to e.g. correctly distinguish parts of a liver from parts of other
organs.
U-Net Cascade To address this practical shortcoming of a 3D U-Net on
datasets with large image sizes, we additionally propose a cascaded model. Therefore, a 3D U-Net is first trained on downsampled images (stage 1). The segmentation results of this U-Net are then upsampled to the original voxel spacing and
passed as additional (one hot encoded) input channels to a second 3D U-Net,
which is trained on patches at full resolution (stage 2). See Figure 1.
full res. image low res. seg. full res. seg. up/downsampling cropping
stage 1 stage 2
skip conn.
Fig. 1. U-Net Cascade (on applicable datasets only). Stage 1 (left): a 3D U-Net processes downsampled data, the resulting segmentation maps are upsampled to the original resolution. Stage 2 (right): these segmentations are concatenated as one-hot encodings to the full resolution data and refined by a second 3D U-Net.
Dynamic adaptation of network topologies Due to the large differences
in image size (median shape 482 × 512 × 512 for Liver vs. 36 × 50 × 35 for
Hippocampus) the input patch size and number of pooling operations per axis
(and thus implicitly the number of convolutional layers) must be automatically
adapted for each dataset to allow for adequate aggregation of spatial information.
Apart from adapting to the image geometries, there are technical constraints like
the available memory to account for. Our guiding principle in this respect is to
dynamically trade off the batch-size versus the network capacity, presented in
detail below:
We start out with network configurations that we know to be working with
our hardware setup. For the 2D U-Net this configuration is an input patch size of
256×256, a batch size of 42 and 30 feature maps in the highest layers (number of
feature maps doubles with each downsampling). We automatically adapt these
parameters to the median plane size of each dataset (where we use the plane
with the lowest in-plane spacing, corresponding to the highest resolution), so
that the network effectively trains on entire slices. We configure the networks to
pool along each axis until the feature map size for that axis is smaller than 8 (but
not more than a maximum of 6 pooling operations). Just like the 2D U-Net, our
3D U-Net uses 30 feature maps at the highest resolution layers. Here we start
with a base configuration of input patch size 128 × 128 × 128, and a batch size
of 2. Due to memory constraints, we do not increase the input patch volume
beyond 1283 voxels, but instead match the aspect ratio of the input patch size
to that of the median size of the dataset in voxels. If the median shape of the
dataset is smaller than 1283
then we use the median shape as input patch size
and increase the batch size (so that the total number of voxels processed is the
same as with 128 × 128 × 128 and a batch size of 2). Just like for the 2D U-Net
we pool (for a maximum of 5 times) along each axis until the feature maps have
size 8.
For any network we limit the total number of voxels processed per optimizer
step (defined as the input patch volume times the batch size) to a maximum of
2D U-Net 3D U-Net 3D U-Net lowres
BrainTumour
median patient shape 169x138 138x169x138 -
input patch size 192x160 128x128x128 -
batch size 89 2 -
num pool per axis 5, 5 5, 5, 5 -
Heart
median patient shape 320x232 115x320x232 58x160x116
input patch size 320x256 80x192x128 64x160x128
batch size 33 2 2
num pool per axis 6, 6 4, 5, 5 4, 5, 5
Liver
median patient shape 512x512 482x512x512 121x128x128
input patch size 512x512 128x128x128 128x128x128
batch size 10 2 2
num pool per axis 6, 6 5, 5, 5 5, 5, 5
Hippocampus
median patient shape 50x35 36x50x35 -
input patch size 56x40 40x56x40 -
batch size 366 9 -
num pool per axis 3, 3 3, 3, 3 -
Prostate
median patient shape 320x319 20x320x319 -
input patch size 320x320 20x192x192 -
batch size 26 4 -
num pool per axis 6, 6 2, 5, 5 -
Lung
median patient shape 512x512 252x512x512 126x256x256
input patch size 512x512 112x128x128 112x128x128
batch size 10 2 2
num pool per axis 6, 6 4, 5, 5 4, 5, 5
Pancreas
median patient shape 512x512 96x512x512 96x256x256
input patch size 512x512 96x160x128 96x160x128
batch size 10 2 2
num pool per axis 6, 6 4, 5, 5 4, 5, 5
Table 1. Network topologies as automatically generated for the seven phase 1 tasks
of the Medical Segmentation Decathlon challenge. 3D U-Net lowres refers to the first
stage of the U-Net Cascade. The configuration of the second stage of the U-Net Cascade
is identical to the 3D U-Net.
5% of the dataset. For cases in excess, we reduce the batch size (with a lowerbound of 2).
All network topologies generated for the phase 1 datasets are presented in
table 2.1.
2.2 Preprocessing
The preprocessing is part of the fully automated segmentation pipeline that
our method consists of and, as such, the steps presented below are carried out
without any user intervention.
Cropping All data is cropped to the region of nonzero values. This has no effect
on most datasets such as liver CT, but will reduce the size (and therefore the
computational burden) of skull stripped brain MRI.
Resampling CNNs do not natively understand voxel spacings. In medical images, it is common for different scanners or different acquisition protocols to
result in datasets with heterogeneous voxel spacings. To enable our networks
to properly learn spatial semantics, all patients are resampled to the median
voxel spacing of their respective dataset, where third order spline interpolation
is used for image data and nearest neighbor interpolation for the corresponding
segmentation mask.
Necessity for the U-Net Cascade is determined by the following heuristics:
If the median shape of the resampled data has more than 4 times the voxels
that can be processed as input patch by the 3D U-Net (with a batch size of
2), it qualifies for the U-Net Cascade and this dataset is additionally resampled
to a lower resolution. This is done by increasing the voxel spacing (decrease
resolution) by a factor of 2 until the above mentioned criterion is met. If the
dataset is anisotropic, the higher resolution axes are first downsampled until
they match the low resolution axis/axes and only then all axes are downsampled
simultaneously. The following datasets of phase 1 fall within the set of described
heuristics and hence trigger usage of the U-Net Cascade: Heart, Liver, Lung,
and Pancreas.
Normalization Because the intensity scale of CT scans is absolute, all CT
images are automatically normalized based on statistics of the entire respective
dataset: If the modality description in a dataset’s corresponding json desccriptor
file indicates ‘ct’, all intensity values occurring within the segmentation masks
of the training dataset are collected and the entire dataset is normalized by
clipping to the [0.5, 99.5] percentiles of these intensity values, followed by a zscore normalization based on the mean and standard deviation of all collected
intensity values. For MRI or other image modalities (i.e. if no ‘ct’ string is
found in the modality), simple z-score normalization is applied to the patient
individually.
If cropping reduces the average size of patients in a dataset (in voxels) by
1/4 or more the normalization is carried out only within the mask of nonzero
elements and all values outside the mask are set to 0.
2.3 Training Procedure
All models are trained from scratch and evaluated using five-fold cross-validation
on the training set. We train our networks with a combination of dice and crossentropy loss:
Ltotal = Ldice + LCE (1)
For 3D U-Nets operating on nearly entire patients (first stage of the U-Net
Cascade and 3D U-Net if no cascade is necessary) we compute the dice loss for
each sample in the batch and average over the batch. For all other networks we
interpret the samples in the batch as a pseudo-volume and compute the dice loss
over all voxels in the batch.
The dice loss formulation used here is a multi-class adaptation of the variant
proposed in [14]. Based on past experience [13,1] we favor this formulation over
other variants [8,15]. The dice loss is implemented as follows:
Ldc = −
2
|K|
X
k∈K
P
i∈I
u
k
i
v
k
P
i
i∈I
u
k
i +
P
i∈I
v
k
i
(2)
where u is the softmax output of the network and v is a one hot encoding
of the ground truth segmentation map. Both u and v have shape I × K with
i ∈ I being the number of pixels in the training patch/batch and k ∈ K being
the classes.
We use the Adam optimizer with an initial learning rate of 3 × 10−4
for
all experiments. We define an epoch as the iteration over 250 training batches.
During training, we keep an exponential moving average of the validation (l
v
MA)
and training (l
t
MA) losses. Whenever l
t
MA did not improve by at least 5 × 10−3
within the last 30 epochs, the learning rate was reduced by factor 5. The training
was terminated automatically if l
v
MA did not improve by more than 5 × 10−3
within the last 60 epochs, but not before the learning rate was smaller than
10−6
.
Data Augmentation When training large neural networks from limited training data, special care has to be taken to prevent overfitting. We address this problem by utilizing a large variety of data augmentation techniques. The following
augmentation techniques were applied on the fly during training: random rotations, random scaling, random elastic deformations, gamma correction augmentation and mirroring. Data augmentation was done with our own in-house framework which is publically available at github.com/MIC-DKFZ/batchgenerators.
We define sets of data augmentation parameters for the 2D and 3D U-Net
separately. These parameters are not modified between datasets.
Applying three dimensional data augmentation may be suboptimal if the
maximum edge length of the input patch size of a 3D U-Net is more than two
times as large as the shortest. For datasets where this criterion applies we use
our 2D data augmentation instead and apply it slice-wise for each sample.
The second stage of the U-Net Cascade receives the segmentations of the
previous step as additional input channels. To prevent strong co-adaptation we
apply random morphological operators (erode, dilate, open, close) and randomly
remove connected components of these segmentations.
Patch Sampling To increase the stability of our network training we enforce
that more than a third of the samples in a batch contain at least one randomly
chosen foreground class.
2.4 Inference
Due to the patch-based nature of our training, all inference is done patch-based
as well. Since network accuracy decreases towards the border of patches, we
weigh voxels close to the center higher than those close to the border, when
aggregating predictions across patches. Patches are chosen to overlap by patch
size / 2 and we further make use of test time data augmentation by mirroring
all patches along all valid axes.
Combining the tiled prediction and test time data augmentation result in
segmentations where the decision for each voxel is obtained by aggregating up
to 64 predictions (in the center of a patient using 3D U-Net). For the test cases
we use the five networks obtained from our training set cross-validation as an
ensemble to further increase the robustness of our models.
2.5 Postprocessing
A connected component analysis of all ground truth segmentation labels is performed on the training data. If a class lies within a single connected component
in all cases, this behaviour is interepreted as a general property of the dataset.
Hence, all but the largest connected component for this class are automatically
removed on predicted images of the corresponding dataset.
2.6 Ensembling and Submission
To further increase the segmentation performance and robustness all possible
combinations of two out of three of our models are ensembled for each dataset.
For the final submission, the model (or ensemble) that achieves the highest mean
foreground dice score on the training set cross-validation is automatically chosen.
3 Experiments and Results
We optimize our network topologie using five-fold cross-validations on the phase
1 datasets. Our phase 1 cross-validation results as well as the corresponding
submitted test set results are summarized in Table 2. - indicates that the U-Net
Cascade was not applicable (i.e. necessary, according to our criteria) to a dataset
because it was already fully covered by the input patch size of the 3D U-Net. The
model that was used for the final submission is highlighted in bold. Although
several test set submissions were allowed by the platform, we believe it to be
bad practice to do so. Hence we only submitted once and report the results of
this single submission.
As can be seen in Table 2 our phase 1 cross-validation results are robustly
recovered on the held-out test set indicating a desired absence of over-fitting.
The only dataset that suffers from a dip in performance on all of its foreground
classes is BrainTumour. The data of this phase 1 dataset stems from the BRATS
challenge [16] for which such performance drops between validation and testing
are a common sight and attributed to a large shift in the respective data and/or
ground-truth distributions.
4 Discussion
In this paper we present the nnU-Net segmentation framework for the medical domain that directly builds around the original U-Net architecture [6] and
dynamically adapts itself to the specifics of any given dataset. Based on our hypothesis that non-architectural modifications can be much more powerful than
BrainTumour Heart Liver Hippoc. Prostate Lung Pancreas
label 1 2 3 1 1 2 1 2 1 2 1 1 2
2D U-Net 78.60 58.65 77.42 91.36 94.37 53.94 88.52 86.70 61.98 84.31 52.68 74.70 35.41
3D U-Net 80.71 62.22 79.07 92.45 94.11 61.74 89.87 88.20 60.77 83.73 55.87 77.69 42.69
3D U-Net
stage1 only
(U-Net Cascade)
- - - 90.63 94.69 47.01 - - - - 65.33 79.45 49.65
3D U-Net
(U-Net Cascade) - - - 92.40 95.38 58.49 - - - - 66.85 79.30 52.12
ensemble
2D U-Net+
3D U-Net
80.79 61.72 79.16 92.70 94.30 60.24 89.78 88.09 63.78 85.31 55.96 78.26 40.46
ensemble
2D U-Net+
3D U-Net
(U-Net Cascade)
- - - 92.64 95.31 60.09 - - - - 61.18 78.79 45.46
ensemble
3D U-Net+
3D U-Net
(U-Net Cascade)
- - - 92.63 95.43 61.82 - - - - 65.16 79.70 49.14
test set 67.71 47.73 68.16 92.77 95.24 73.71 90.37 88.95 75.81 89.59 69.20 79.53 52.27
Table 2. Mean dice scores for the proposed models in all phase 1 tasks. All experiments
were run as five-fold cross-validation. The models that we used for generating our test
set submission are highlighted in bold. The dice scores of the test sets are shown at
the bottom of the table. Test dice scores in bold denote that at the time of manuscript
submission these scores were the highest in the online leaderboard of the challenge
(decathlon.grand-challenge.org/evaluation/results).
some of the recently presented architectural modifications, the essence of this
framework is a thorough design of adaptive preprocessing, training scheme and
inference. All design choices required to adapt to a new segmentation task are
done in a fully automatic manner with no manual interaction. For each task
the nnU-Net automatically runs a five-fold cross-validation for three different
automatically configures U-Net models and the model (or ensemble) with the
highest mean foreground dice score is chosen for final submission. In the context of the Medical Segmentation Decathlon we demonstrate that the nnU-Net
performs competitively on the held-out test sets of 7 highly distinct medical
datasets, achieving the highest mean dice scores for all classes of all tasks (except class 1 in the BrainTumour dataset) on the online leaderboard at the time of
manuscript submission. We acknowledge that training three models and picking
the best one for each dataset independently is not the cleanest solution. Given
a larger time-scale, one could investigate proper heuristics to identify the best
model for a given dataset prior to training. Our current tendency favors the
U-Net Cascade (or the 3D U-Net if the cascade cannot be applied) with the sole
(close) exceptions being the Prostate and Liver tasks. Additionally, the added
benefit of many of our design choices, such as the use of Leaky ReLUs instead of
regular ReLUs and the parameters of our data augmentation were not properly
validated in the context of this challenge. Future work will therefore focus on
systematically evaluating all design choices via ablation studies.
References
1. F. Isensee, P. Kickingereder, W. Wick, M. Bendszus, and K. H. Maier-Hein,
“Brain Tumor Segmentation and Radiomics Survival Prediction: Contribution to
the BRATS 2017 Challenge,” in International MICCAI Brainlesion Workshop.
Springer, 2017, pp. 287–297.
2. X. Li, H. Chen, X. Qi, Q. Dou, C.-W. Fu, and P. A. Heng, “H-DenseUNet: Hybrid
densely connected UNet for liver and liver tumor segmentation from CT volumes,”
arXiv preprint arXiv:1709.07330, 2017.
3. A. G. Roy, N. Navab, and C. Wachinger, “Concurrent Spatial and Channel Squeeze
& Excitation in Fully Convolutional Networks,” arXiv preprint arXiv:1803.02579,
2018.
4. O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori,
S. McDonagh, N. Y. Hammerla, B. Kainz et al., “Attention U-Net: Learning Where
to Look for the Pancreas,” arXiv preprint arXiv:1804.03999, 2018.
5. S. J´egou, M. Drozdzal, D. Vazquez, A. Romero, and Y. Bengio, “The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation,”
in Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE
Conference on. IEEE, 2017, pp. 1175–1183.
6. O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for
biomedical image segmentation,” in MICCAI. Springer, 2015, pp. 234–241.
7. K. He, X. Zhang, S. Ren, and J. Sun, “Identity mappings in deep residual networks,” in ECCV. Springer, 2016, pp. 630–645.
8. F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional neural networks for volumetric medical image segmentation,” in International Conference on
3D Vision. IEEE, 2016, pp. 565–571.
9. J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation,” in Proceedings of the IEEE conference on computer vision and
pattern recognition, 2015, pp. 3431–3440.
10. L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:
Semantic image segmentation with deep convolutional nets, atrous convolution,
and fully connected crfs,” IEEE transactions on pattern analysis and machine
intelligence, vol. 40, no. 4, pp. 834–848, 2018.
11. D. Ulyanov, A. Vedaldi, and V. Lempitsky, “Instance normalization: The missing
ingredient for fast stylization,” arXiv preprint arXiv:1607.08022, 2016.
12. S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training
by reducing internal covariate shift,” arXiv preprint arXiv:1502.03167, 2015.
13. F. Isensee, P. F. Jaeger, P. M. Full, I. Wolf, S. Engelhardt, and K. H. Maier-Hein,
“Automatic cardiac disease assessment on cine-mri via time-series segmentation
and domain specific features,” in International Workshop on Statistical Atlases
and Computational Models of the Heart. Springer, 2017, pp. 120–129.
14. M. Drozdzal, E. Vorontsov, G. Chartrand, S. Kadoury, and C. Pal, “The importance of skip connections in biomedical image segmentation,” in Deep Learning
and Data Labeling for Medical Applications. Springer, 2016, pp. 179–187.
15. C. H. Sudre, W. Li, T. Vercauteren, S. Ourselin, and M. J. Cardoso, “Generalised
Dice overlap as a deep learning loss function for highly unbalanced segmentations,”
in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical
Decision Support. Springer, 2017, pp. 240–248.
16. B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby,
Y. Burren, N. Porz, J. Slotboom, R. Wiest et al., “The multimodal brain tumor
image segmentation benchmark (BRATS),” IEEE TMI, vol. 34, no. 10, pp. 1993–
2024, 2015