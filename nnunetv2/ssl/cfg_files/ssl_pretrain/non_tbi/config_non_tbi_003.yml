cropping:
  tile_size: 24
  stride_d: 12

training:
    # Data  -------------------------------------------------------------

    # Default: (96, 160, 160) (nnUnet) (16, 96, 96) (DeSD)
    global_crop_size: [16, 112, 112]
    wise_crop: False
    # Default: (16, 48, 48)
    # Scale range of the cropped image before resizing, relatively to the origin image.    
    local_crop_size: [16, 48, 48]
    # Default: True 
    fixed_slabs: False
    # Default: null 
    n_iters: 9600
    n_val_iters: 3200
    # Default: null
    already_cropped: True
    bboxes_path: '/home/jseia/Desktop/thesis/code/stroke-seg/experiments/crop_brain_vols/bbox_info.json'
    # Default: null
    slab_thickness: 24
    all_datasets_csv_path: '/home/jseia/Desktop/thesis/code/stroke-seg/data/dataset.csv'
    # Not valid for symmetry augm
    multichannel_input: False
    # Transformations. Default:
    # scale: [0.8, 1], mirror: 0.8, symmetry: 0.5, g_noise: True, g_blur: 0.8, mult_bright: 0.8
    # brightness: 0.8, contrast_augm: True, gamma: 0.8
    transformations_cfg:
      # Used for large global view cropping. When disabling multi-crop (local_crops_number: 0), we
      # recommand using a wider range of scale ("global_crops_scale 0.14 1." for example)
      # Default: (0.5, 0.7). DeSD ->(0.8, 1.2)
      global_scale: [0.8, 1.2]
      # Scale range of the cropped image before resizing, relatively to the origin image.
      # Used for small local view cropping of multi-crop. Default: (0.8, 1.0)
      local_scale: [1.5, 2.0]
      mirror: True
      symmetry: 0.7
      i_global:
        g_noise: [1.0, 1.0]
        g_blur: [0.8, 0.8]
        mult_bright: [0.8, 1.0]
        brightness: [0.8, 0.8]
        contrast_augm: [1.0, 1.0]
        gamma: [0.8, 1.0]
        per_channel: False
      i_local:
        g_noise: [1.0, 1.0]
        g_blur: [0.8, 0.5]
        mult_bright: [1.0, 1.0]
        brightness: [0.8, 0.5]
        contrast_augm: [1.0, 1.0]
        gamma: [1.0, 1.0]
        per_channel: True

    # Multi-crop parameters ------------------------------------------------------------
    # Number of small local views to generate. Set this parameter to 0 to disable multi-crop
    # training. Default: 0
    local_crops_number: 0
        
    # Model Parameters -------------------------------------------------------------

    # Dimensionality of the DeSD head output. Default: 65536
    out_dim: 65536
    
    # Whether or not to weight normalize the last layer of the DeSD head.
    # Not normalizing leads to better performance but can make the training unstable.
    # Default: True
    norm_last_layer: True
    
    #'Base EMA parameter for teacher update. The value is increased to 1 during training
    # with cosine schedule. We recommend setting a higher value with small batches.
    # Default: 0.996
    momentum_teacher: 0.996
  
    # Whether to use batch normalizations in projection head (Default: False)
    use_bn_in_head: True

    # Temperature teacher parameters -------------------------------
    
    # Initial value for the teacher temperature: 0.04 works well in most cases.
    # Try decreasing it if the training loss does not decrease.  Default: 0.04
    warmup_teacher_temp: 0.04

    # Final value (after linear warmup) of the teacher temperature. For most experiments,
    # anything above 0.07 is unstable. We recommend starting with the default value of 0.04
    # and increase this slightly if needed.  Default: 0.04
    teacher_temp: 0.07

    # Number of warmup epochs for the teacher temperature.  Default: 0
    warmup_teacher_temp_epochs: 10

    # Training/Optimization parameters -----------------------------------

    # Whether or not to use half precision for training. Improves training time and memory
    # requirements, but can provoke instability and slight decay of performance. We
    # recommend disabling mixed precision if the loss is unstable, if reducing the patch
    # size or if training with bigger models.  Default: True
    use_fp16: True
    
    # Initial value of the weight decay. With ViT, a smaller value at the beginning
    # of training works well.  Default: 0.04
    weight_decay: 0.04

    # Final value of the weight decay. We use a cosine schedule for WD and using a larger decay by
    # the end of training improves performance for ViTs. Default: 0.4
    weight_decay_end: 0.000001

    # Maximal parameter gradient norm if using gradient clipping. Clipping with norm .3 ~ 1.0 can
    # help optimization for larger ViT architectures. 0 for disabling. Default 3.0
    clip_grad: 0
    
    # Batch-size. Default: 32
    batch_size: 64
    
    # Epochs. Default: 100
    epochs: 100
    
    # Number of epochs during which we keep the output layer fixed. Typically doing so during
    # the first epoch helps training. Try increasing this value if the loss does not decrease.
    # Default: 1
    freeze_last_layer: 1
    
    # Learning rate at the end of linear warmup (highest LR used during training). The learning
    # rate is linearly scaled with the batch size. Default: 0.0005
    lr: 0.1

    # Batch size realted parameter: Default: 256. From the original paper: 8*32 / 256
    lr_sch_den: 64
    
    # Number of epochs for the linear learning-rate warm up. Default: 10
    warmup_epochs: 10
    
    # Target LR at the end of optimization. We use a cosine LR schedule with linear warmup.
    # Default: 1e-6 / DeSD = 0.048
    min_lr: 0.01

    optimizer: 'adamw'
    # Options: 'adamw', 'sgd', 'lars'. Type of optimizer.
    # We recommend using adamw with ViTs. Default: 'adamw'

    weights: 'constant'

    # Misc -------------------------------------------------------
    # Path to save logs and checkpoints.
    output_dir: '/home/jseia/Desktop/thesis/experiments/non_tbi/003'
    # Save checkpoint every x epochs.
    saveckp_freq: 50
    # Random seed.
    seed: 0
    # Number of data loading workers
    num_workers: 6

    # Metrics_cfg ----------------------------------------------------------------
    metrics_cfg:
      over: ['val']
      # Frequency to log the dimensionality reduced projection scatter plot, null if avoided
      rank_me_freq: 1
      # Frequency to log the dimensionality reduced projection scatter plot, null if avoided
      projection_freq: 5

    #nnUnet ----------------------------------------------------------
    dataset: 'Dataset044_AIS'
    configuration: '3d_fullres'
    fold: 0
    trainer: 'nnUNetTrainerCfg'
    exp_planner: 'nnUNetPlansSSL'
    device: 'cuda'
    num_epochs: 30
    unfreeze_lr: null
    unfreeze_epoch: null
    ssl_pretrained: True
    split_path: '/home/jseia/Desktop/thesis/code/nnUNet_ais/nnunetv2/preprocessed/Dataset044_AIS/splits_final_non_tbi.json'